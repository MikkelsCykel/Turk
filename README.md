# Amazon Mechanical Turk Investigation

## Project contains:

+ Django Website connected with the MTurk API
+ Jupyter Notebook explorring the resulting data
+ Article explaining the findings

## Project Abstract

Crowdsourcing provides a platform for outsourcing of tasks of computational hard nature to a great mob of human work- ers. A straightforward use case of this platform is labelling of data. However, as the tasks are distributed to a vary- ing independent set of workers, the skill-set and knowledge of the working crowd will change from task to task. This drastically increases the complexity of assuring high quality of the task completion, while also making the cost per task somewhat incalculable.
This study seeks to address this varying quality phenom- ena within crowd-sourcing. In specific, this research will try to formulate a model optimised for consistent task quality while being cost effective. Moreover, the model must be au- tomatable. The platform subject to this study will is the Amazon Mechanical Turk crowd-sourcing platform.
The paper proposes a straight forward ’get-focused’ ap- proach, which features a preliminary task aiming to force the worker to inspect the task at hand in detail. Through A-B split test of 3444 investigations, the approach described a quality improvement of up to 20 percent.

## Project Data

The data subject to this analysis is a set of 313 images containing food.
